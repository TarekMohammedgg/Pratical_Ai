{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using model from openai or google-gemeni or claude.ai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\root\\Programming\\Pratical_Ai\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### connect with tht chat model \n",
    "### to add more security to info like api_keys \n",
    "from dotenv import dotenv_values \n",
    "env_value = dotenv_values('app.env') \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI \n",
    "model = ChatGoogleGenerativeAI(api_key=env_value['API_KEY'] , model = \"gemini-1.5-flash\") \n",
    "### we can add temprature attribute in model if 1 the model is so creative , if 0 the model is more restricted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.messages import HumanMessage \n",
    "# message = HumanMessage(content=[\n",
    "#     {\"type\" : \"text\"  , \n",
    "#      \"text\" : \"what is your name ? \"} \n",
    "# ])\n",
    "message = \"what is the meaning of machine learning ? \".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='## Machine Learning:  Teaching Machines to Learn\\n\\nMachine learning (ML) is a field of artificial intelligence (AI) where computers learn from data without being explicitly programmed. Instead of being given a set of instructions, ML algorithms are trained on data to identify patterns and make predictions. \\n\\nHere\\'s a simplified explanation:\\n\\n**Imagine you\\'re teaching a child to recognize a dog.**\\n\\n* **Traditional programming:** You\\'d give the child a list of characteristics: four legs, furry, barks, tail wags.\\n* **Machine learning:** You\\'d show the child pictures of dogs and tell them \"This is a dog.\" The child would then analyze the pictures and learn to identify dogs based on common features.\\n\\n**Key Concepts in Machine Learning:**\\n\\n* **Data:** The fuel for machine learning. The more data, the better the algorithms learn.\\n* **Algorithms:** The mathematical models that analyze data and learn from it.\\n* **Training:** The process of feeding data to the algorithm and allowing it to learn.\\n* **Prediction:** Once trained, the algorithm can make predictions about new data.\\n\\n**Types of Machine Learning:**\\n\\n* **Supervised learning:** The algorithm is trained on labeled data, where each example has a known output. This is like the \"dog\" example above.\\n* **Unsupervised learning:** The algorithm is trained on unlabeled data, and it must find patterns on its own. This is like grouping similar objects together without knowing their names.\\n* **Reinforcement learning:** The algorithm learns through trial and error, receiving rewards for correct actions and penalties for incorrect ones. This is like teaching a robot to play a game.\\n\\n**Applications of Machine Learning:**\\n\\nMachine learning is used in a wide range of applications, including:\\n\\n* **Image recognition:** Identifying objects in images, like facial recognition and medical imaging.\\n* **Natural language processing:** Understanding and generating human language, like chatbots and machine translation.\\n* **Recommendation systems:** Suggesting products or content based on user preferences, like Netflix and Amazon.\\n* **Fraud detection:** Identifying suspicious transactions in financial systems.\\n* **Self-driving cars:** Navigating roads and making decisions based on real-time data.\\n\\n**In summary:** Machine learning is a powerful tool that allows computers to learn from data and solve complex problems. It\\'s transforming various industries and making our lives easier and more efficient.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-7719e58b-b65c-42b9-8ff7-6ef10791df3f-0', usage_metadata={'input_tokens': 9, 'output_tokens': 496, 'total_tokens': 505})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseChatModel._combine_llm_outputs of ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000002672BC68920>, default_metadata=())>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### run multiple prompts to gemini \n",
    "from langchain_core.messages import HumanMessage \n",
    "from langchain.chains import LLMChain , sequential \n",
    "message = [(\"what is your name ? \") , (\"what is your job ? \")] \n",
    "model.invoke(message[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Please tell me what you want me to explain! I need to know what you\\'re curious about. \\n\\nFor example, you could ask:\\n\\n* \"Explain how a car works\"\\n* \"Explain what a rainbow is\"\\n* \"Explain what a dinosaur is\"\\n\\nOnce you tell me what you want to know, I can explain it in a way that a 5-year-old can understand. üòä \\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-2f922c76-af2c-487d-aa81-7b0abf94cd34-0', usage_metadata={'input_tokens': 9, 'output_tokens': 87, 'total_tokens': 96})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use prompt templates \n",
    "from langchain.prompts import PromptTemplate \n",
    "\n",
    "string_prompt = PromptTemplate.from_template(\"Explain {subject} as i am 5 \") \n",
    "\n",
    "user_subject = input(\"enter what is the subject you want ? \") \n",
    "prompt = string_prompt.format(subject = user_subject )\n",
    "model.invoke(prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ÿ™ÿÆŸäŸÑ ÿ£ŸÜŸÉ ÿ™ŸèÿπŸÑŸÖ ŸÉŸÑÿ®Ÿãÿß ÿ¨ÿØŸäÿØŸãÿß ÿ®ÿπÿ∂ ÿßŸÑÿ≠ŸäŸÑ. ÿ™Ÿèÿ±ŸäŸá ÿßŸÑŸÉÿ±ÿ©ÿå Ÿàÿ™ŸÇŸàŸÑ \"ÿßÿ¨ŸÑÿ®Ÿáÿß!\"ÿå ÿ´ŸÖ ÿ™Ÿèÿπÿ∑ŸäŸáÿß ŸÑŸá ÿπŸÜÿØŸÖÿß Ÿäÿ¨ŸÑÿ®Ÿáÿß. ŸÖÿπ ŸÖÿ±Ÿàÿ± ÿßŸÑŸàŸÇÿ™ÿå Ÿäÿ®ÿØÿ£ ÿßŸÑŸÉŸÑÿ® ÿ®ŸÅŸáŸÖ ÿ£ŸÜ \"ÿßÿ¨ŸÑÿ®Ÿáÿß!\" ÿ™ÿπŸÜŸä ÿ£ŸÜ ÿπŸÑŸäŸá ÿ£ŸÜ Ÿäÿ¨ŸÑÿ® ÿßŸÑŸÉÿ±ÿ©. \\n\\nÿßŸÑÿ¢ŸÜÿå ÿ™ÿÆŸäŸÑ ÿ£ŸÜ ŸÑÿØŸäŸÉ ÿ¨Ÿáÿßÿ≤ ŸÉŸÖÿ®ŸäŸàÿ™ÿ± ŸäŸèÿπŸÑŸÖ ŸÜŸÅÿ≥Ÿá ÿßŸÑÿ≠ŸäŸÑ! Ÿáÿ∞ÿß ŸáŸà ŸÖÿß ŸäŸÅÿπŸÑŸá **ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ¢ŸÑŸä**. \\n\\nŸäÿπÿ∑ŸäŸÜÿß ÿßŸÑÿ¨Ÿáÿßÿ≤ ÿßŸÑŸÉŸÖÿ®ŸäŸàÿ™ÿ± ÿßŸÑŸÉÿ´Ÿäÿ± ŸÖŸÜ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ÿå ŸÖÿ´ŸÑ ÿßŸÑÿµŸàÿ± ŸàÿßŸÑÿ±ÿ≥ÿßÿ¶ŸÑÿå ŸàŸäŸèÿπŸÑŸÖ ŸÜŸÅÿ≥Ÿá ŸÖŸÜ ÿÆŸÑÿßŸÑŸáÿß. ŸÖÿ´ŸÑ ÿßŸÑŸÉŸÑÿ®ÿå Ÿäÿ®ÿØÿ£ ÿßŸÑŸÉŸÖÿ®ŸäŸàÿ™ÿ± ÿ®ŸÅŸáŸÖ ÿßŸÑÿ£ŸÜŸÖÿßÿ∑ ŸÅŸä ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸàŸäÿ≥ÿ™ÿ∑Ÿäÿπ ÿ®ÿπÿØ ÿ∞ŸÑŸÉ ÿßŸÑÿ™ŸÜÿ®ÿ§ ÿ®ÿ£ÿ¥Ÿäÿßÿ° ÿ¨ÿØŸäÿØÿ©. \\n\\nÿπŸÑŸâ ÿ≥ÿ®ŸäŸÑ ÿßŸÑŸÖÿ´ÿßŸÑÿå ŸäŸÖŸÉŸÜŸÜÿß ÿ£ŸÜ ŸÜŸèÿπŸÑŸÖ ÿ¨Ÿáÿßÿ≤ ŸÉŸÖÿ®ŸäŸàÿ™ÿ± ÿßŸÑÿ™ÿπÿ±ŸÅ ÿπŸÑŸâ ÿßŸÑŸÇÿ∑ÿ∑ ŸÅŸä ÿßŸÑÿµŸàÿ±. ŸÜŸèÿ±ŸäŸá ÿßŸÑŸÉÿ´Ÿäÿ± ŸÖŸÜ ÿµŸàÿ± ÿßŸÑŸÇÿ∑ÿ∑ÿå ŸàŸäŸèÿπŸÑŸÖ ŸÜŸÅÿ≥Ÿá ŸÖŸÜ ÿÆŸÑÿßŸÑŸáÿß ŸÖÿß ŸáŸä ŸÖŸÑÿßŸÖÿ≠ ÿßŸÑŸÇÿ∑ÿ©. ÿ®ÿπÿØ ÿ∞ŸÑŸÉÿå ŸäŸÖŸÉŸÜŸá ÿßŸÑÿ™ÿπÿ±ŸÅ ÿπŸÑŸâ ÿßŸÑŸÇÿ∑ÿ∑ ŸÅŸä ÿµŸàÿ± ÿ¨ÿØŸäÿØÿ© ŸÑŸÖ Ÿäÿ±Ÿáÿß ŸÖŸÜ ŸÇÿ®ŸÑ!\\n\\nŸáÿ∞ÿß ŸÖÿß ŸäŸÅÿπŸÑŸá **ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ¢ŸÑŸä**ÿå ŸäŸèÿπŸÑŸÖ ŸÜŸÅÿ≥Ÿá ŸÖŸÜ ÿÆŸÑÿßŸÑ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸàŸäŸèÿµÿ®ÿ≠ ÿ∞ŸÉŸäŸãÿß ŸÖÿ´ŸÑ ÿßŸÑŸÉŸÑÿ®!\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### another way  for prompt template \n",
    "string_prompt = PromptTemplate(template= \"explain {subject} as i am 5 , write the answer in {language}\" , \n",
    "                               input_variables= ['subject' , 'language']) ## input_variables is optional \n",
    "user_subject = \"machine learning \" \n",
    "user_language = 'arabic' \n",
    "prompt = string_prompt.format(subject = user_subject , language = user_language) \n",
    "model.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ÿ™ÿÆŸäŸÑ Ÿáÿ±ŸÖŸãÿß ŸÉÿ®Ÿäÿ±Ÿãÿßÿå ŸÖÿ´ŸÑ Ÿáÿ±ŸÖ ÿßŸÑŸÅÿ±ÿßÿπŸÜÿ© ŸÅŸä ŸÖÿµÿ±! \\n\\n* **ÿßŸÑŸÇÿßÿπÿØÿ©** ŸáŸä ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ£Ÿàÿ≥ÿπ ŸÅŸä ÿßŸÑŸáÿ±ŸÖÿå  Ÿàÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ÿßŸÑÿ£ÿ∑ÿπŸÖÿ© ÿßŸÑÿ™Ÿä Ÿäÿ¨ÿ® ÿ£ŸÜ ŸÜÿ£ŸÉŸÑŸáÿß ÿ£ŸÉÿ´ÿ±ÿå ŸÖÿ´ŸÑ **ÿßŸÑÿÆÿ∂ÿßÿ± ŸàÿßŸÑŸÅŸàÿßŸÉŸá**. \\n* **ÿßŸÑÿ∑ÿ®ŸÇÿ© ÿßŸÑÿ´ÿßŸÜŸäÿ©** ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ **ÿßŸÑÿ≠ÿ®Ÿàÿ®** ŸÖÿ´ŸÑ ÿßŸÑÿÆÿ®ÿ≤ ŸàÿßŸÑÿ±ÿ≤ ŸàÿßŸÑÿ£ÿ±ÿ≤ ÿßŸÑÿ®ŸÜŸä.\\n* **ÿßŸÑÿ∑ÿ®ŸÇÿ© ÿßŸÑÿ´ÿßŸÑÿ´ÿ©** ÿ™ÿ≠ÿ™ŸàŸä ÿπŸÑŸâ **ÿßŸÑÿ®ÿ±Ÿàÿ™ŸäŸÜÿßÿ™** ŸÖÿ´ŸÑ ÿßŸÑŸÑÿ≠ŸàŸÖ ŸàÿßŸÑÿØÿ¨ÿßÿ¨ ŸàÿßŸÑÿ£ÿ≥ŸÖÿßŸÉ ŸàÿßŸÑÿ≠ŸÑŸäÿ® ŸàÿßŸÑÿ®Ÿäÿ∂.\\n* **ÿßŸÑÿ∑ÿ®ŸÇÿ© ÿßŸÑÿ±ÿßÿ®ÿπÿ©** ŸáŸä **ÿßŸÑÿØŸáŸàŸÜ** ŸÖÿ´ŸÑ ÿßŸÑÿ≤ÿ®ÿØÿ© ŸàÿßŸÑÿ≤ŸäŸàÿ™ÿå ŸàŸÜÿ≠ÿ™ÿßÿ¨Ÿáÿß ÿ®ŸÉŸÖŸäÿßÿ™ ŸÇŸÑŸäŸÑÿ©.\\n\\nŸÉŸÑ ÿ∑ÿ®ŸÇÿ© ŸÖŸÜ ÿßŸÑŸáÿ±ŸÖ ŸÖŸáŸÖÿ© ŸÑÿµÿ≠ÿ™ŸÜÿßÿå ŸàŸÜÿ≠ÿ™ÿßÿ¨ ÿ•ŸÑŸâ ÿ™ŸÜÿßŸàŸÑ ŸÉŸÑ ÿ£ŸÜŸàÿßÿπ ÿßŸÑÿ∑ÿπÿßŸÖ ŸÖŸÜ ŸÉŸÑ ÿ∑ÿ®ŸÇÿ©ÿå ŸàŸÑŸÉŸÜ ÿ®ŸÉŸÖŸäÿßÿ™ ŸÖÿÆÿ™ŸÑŸÅÿ©. \\n\\nÿ™ÿ∞ŸÉÿ±: \\n* **ŸÉŸÑ ÿßŸÑŸÉÿ´Ÿäÿ± ŸÖŸÜ ÿßŸÑÿÆÿ∂ÿßÿ± ŸàÿßŸÑŸÅŸàÿßŸÉŸá** ŸÑÿ£ŸÜŸáÿß ÿ™ÿ¨ÿπŸÑŸÜÿß ŸÜÿ¥ÿπÿ± ÿ®ÿßŸÑŸÜÿ¥ÿßÿ∑.\\n* **ÿ™ŸÜÿßŸàŸÑ ÿßŸÑÿ≠ÿ®Ÿàÿ®** ŸÑÿ£ŸÜŸáÿß ÿ™ÿπÿ∑ŸäŸÜÿß ÿßŸÑÿ∑ÿßŸÇÿ© ŸÑŸÑÿπÿ®.\\n* **ÿ™ŸÜÿßŸàŸÑ ÿßŸÑÿ®ÿ±Ÿàÿ™ŸäŸÜÿßÿ™** ŸÑÿ£ŸÜŸáÿß ÿ™ÿ≥ÿßÿπÿØŸÜÿß ÿπŸÑŸâ ÿßŸÑŸÜŸÖŸà.\\n* **ÿ™ŸÜÿßŸàŸÑ ÿßŸÑÿØŸáŸàŸÜ** ÿ®ŸÉŸÖŸäÿßÿ™ ŸÇŸÑŸäŸÑÿ© ŸÑÿ£ŸÜŸáÿß ÿ™ÿ≥ÿßÿπÿØŸÜÿß ÿπŸÑŸâ ÿßŸÖÿ™ÿµÿßÿµ ÿßŸÑŸÅŸäÿ™ÿßŸÖŸäŸÜÿßÿ™.\\n\\nÿßŸÑŸáÿ±ŸÖ ÿßŸÑÿ∫ÿ∞ÿßÿ¶Ÿä Ÿäÿ≥ÿßÿπÿØŸÜÿß ÿπŸÑŸâ ÿßÿÆÿ™Ÿäÿßÿ± ÿßŸÑÿ£ÿ∑ÿπŸÖÿ© ÿßŸÑÿµÿ≠Ÿäÿ© ÿßŸÑÿ™Ÿä ÿ™ÿ¨ÿπŸÑŸÜÿß ŸÜÿ¥ÿπÿ± ÿ®ÿßŸÑŸÜÿ¥ÿßÿ∑ ŸàÿßŸÑÿµÿ≠ÿ©. \\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### another way to defince template \n",
    "message_template = '\\n'.join(['ÿßÿ¥ÿ±ÿ≠ ŸÑŸä ' , \n",
    "                              '{subject}' , \n",
    "                              'ŸÉŸÖÿß ŸÑŸà ÿßŸÜŸÜŸä ÿßÿ®ŸÑÿ∫ ÿßŸÑÿÆÿßŸÖÿ≥ÿ© Ÿà ŸÖŸÜ ÿßŸÑÿπŸÖÿ±']) \n",
    "string_prompt = PromptTemplate(template= message_template  )\n",
    "user_subject = \"ÿßŸÑŸáÿ±ŸÖ ÿßŸÑÿ∫ÿ∞ÿßÿ¶Ÿä\" \n",
    "prompt = string_prompt.format(subject = user_subject)\n",
    "model.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ai: content=\"Alright, let's see if we can create your dream dish! Tell me, what are some of your favorite **ingredients**?  Do you have any **allergies** I should be aware of?  \\n\\nDo you prefer **savory** or **sweet** flavors?  What about **textures**?  Do you enjoy **crispy** things, **creamy** things, or maybe **chewy** things?\\n\\nAnd finally, what kind of **mood** are you in?  Are you looking for something **comforting**, **exciting**, or maybe **light and refreshing**? \\n\\nThe more information you give me, the better I can understand your taste buds and create something truly special for you! üòä \\n\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'LOW', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-bc18a418-bb86-4d55-a624-f578d470ffcb-0' usage_metadata={'input_tokens': 19, 'output_tokens': 148, 'total_tokens': 167}\n",
      "Ai: content='Noodles are a great choice!  Tell me, what kind of noodles do you enjoy most?  Are we talking about ramen, spaghetti, udon, or something else entirely? \\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-5464f624-1c3c-4bff-89e6-834935a17e3a-0' usage_metadata={'input_tokens': 22, 'output_tokens': 38, 'total_tokens': 60}\n",
      "Ai: content=\"Okay, noodles and chicken! That's a great start.  To narrow it down, can you tell me what kind of noodles you prefer? Are we talking about spaghetti, ramen, udon, or something else entirely? \\n\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-771435b7-230a-4afb-97d6-4cb2551497a4-0' usage_metadata={'input_tokens': 24, 'output_tokens': 46, 'total_tokens': 70}\n",
      "Ai: content=\"Okay, so you like noodles, chicken, and some rice.  That's a great start!  To help me figure out your perfect plate, tell me:\\n\\n* **What kind of noodles do you prefer?**  Are you thinking spaghetti, ramen, udon, or something else?\\n* **What kind of chicken do you like?**  Would you prefer grilled, fried, or something else? \\n* **What kind of rice do you like?**  Do you prefer white, brown, or maybe something more exotic like jasmine rice? \\n* **Do you have any other preferences?**  For example, do you like your dishes spicy, creamy, or something else? \\n\\nThe more details you can give me, the better I can understand your taste and create a truly delicious plate just for you! \\n\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-c39e2bd3-cf3c-4c8b-8252-2d6d1a69900a-0' usage_metadata={'input_tokens': 28, 'output_tokens': 171, 'total_tokens': 199}\n",
      "Ai : Chicken Fried Rice \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### how to make conversation between chatbot and human \n",
    "from langchain.schema import HumanMessage  , AIMessage \n",
    "base_conversation = [\n",
    "   \"you are a chef, you want to ask me some things to find out my favorite plate\"\n",
    "    \n",
    "]\n",
    "## Note that : google gemini can't handle SystemMessage . \n",
    "ai_response = None\n",
    "user_response = None  \n",
    "while True :  \n",
    "        ai_response = model.invoke(base_conversation) \n",
    "        print(f'Ai: {ai_response}')\n",
    "        user_string = input(\"answer : \")\n",
    "        user_response = HumanMessage(content=user_string)\n",
    "        if(user_string == 'give me palet') : \n",
    "            base_conversation.append(HumanMessage(content= \"And now , which plate do you think  i will like. just type the plate name \"))\n",
    "            final_result =  model.invoke(base_conversation).content\n",
    "            print(f\"Ai : {final_result}\")\n",
    "            \n",
    "            break  \n",
    "        else : \n",
    "            \n",
    "            base_conversation.append(user_response)\n",
    "            result = model.invoke(base_conversation).content          \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country name : Armenia | capital name : Yerevan \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Basic Fewshots learning \n",
    "message = \"\"\"\n",
    "country name : egypt | capital name : cairo \n",
    "country name : Saudi Arabic | capital name : Riyadh \n",
    "country name : Singapore | capital name : singapore\n",
    "country name : Armenia | \n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "print(model.invoke(message).content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country name :  Australia | capital name : Canberra \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate \n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate \n",
    "\n",
    "demo_tempalte = PromptTemplate(template = \"country name : {country} | capital name : {city} \" , \n",
    "                               input_variables=['country' , 'city'] ) \n",
    "\n",
    "examples = [\n",
    "    {\"country\": \"egypt\" , \"city\": \"cairo \"} , \n",
    "    {\"country\": \"Saudi Arabic\" ,  \"city\" :\" Riyadh \"} , \n",
    "    {\"country\": \"Singapore\" ,  \"city\"  : \"singapore\"}\n",
    "]\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=demo_tempalte,\n",
    "    suffix=\"country name :  {country}\",\n",
    "    input_variables=[\"country\"],\n",
    ")\n",
    "user_country = 'Australia'\n",
    "message  = prompt.format(country = user_country)\n",
    "print(model.invoke(message).content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break down the problem step-by-step:\n",
      "\n",
      "1. **Start with the total:** Your family has 50 oranges.\n",
      "\n",
      "2. **Calculate one-fifth:** One-fifth of 50 oranges is (1/5) * 50 = 10 oranges.\n",
      "\n",
      "3. **Subtract one-fifth:**  Subtract 10 oranges from the total: 50 - 10 = 40 oranges.\n",
      "\n",
      "4. **Add ten more:** Add 10 more oranges: 40 + 10 = 50 oranges.\n",
      "\n",
      "**Final Answer:** There will be 50 oranges in the end. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"When I was seven, my sister was twice my age. Now I am seventy years old, how old can my sister be?\",\n",
    "        \"answer\": \"\\n\".join([\n",
    "            \"We will followup some questions to get the answer.\",\n",
    "            \"Follow up: How old was your sister when you were seven?\",\n",
    "            \"Intermediate answer: Twice, which mean 14 years.\",\n",
    "            \"Follow up: What is the difference between your age and your sister's age?\",\n",
    "            \"Intermediate answer: 14 years - 7 years = 7 years.\",\n",
    "            \"Follow up: When you were seventy years old, how old would your sister be?\",\n",
    "            \"Intermediate answer: my age (70) +  The difference between me and my sister's age (7) = 77 years.\",\n",
    "            \"Final Answer: 77 years.\"\n",
    "        ])\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"I have five oranges, and the sum of what my sister and brother have is three times what I have plus thirty-five?\",\n",
    "        \"answer\": \"\\n\".join([\n",
    "            \"We will followup some questions to get the answer.\",\n",
    "            \"Follow up: How many oranges do you have?\",\n",
    "            \"Intermediate answer: 5 oranges.\",\n",
    "            \"Follow up: How many oranges does your sister and brother have?\",\n",
    "            \"Intermediate answer: three times: = 3 * 5 = 15. Also, we need to add thirty-five to them: 15 + 35 = 50 \",\n",
    "            \"Final Answer: 50 oranges.\"\n",
    "        ])\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt_template = PromptTemplate(template= \"question : {question}\\n------\\nAnswer:{answer}\" , input_variables=['question'  ,'answer'] ) \n",
    "\n",
    "prompt = FewShotPromptTemplate(examples = examples , example_prompt=  prompt_template , \n",
    "                               suffix = \"question : {question}\\n---\\n\" , input_variables = ['question'])  \n",
    "\n",
    "message = prompt.format(question=\"Total with what my family is 50 oranges. If we subtract one-fifth of the number from them, and add ten more oranges, how many oranges will there be in the end?\")\n",
    "print(model.invoke(message).content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews: \n",
      "- I bought this computer for work, and it is so slow that I have to wait hours for it to start. \n",
      "- The keyboard is so noisy that I can't use it in the office. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"product\": \"mobile phone\",\n",
    "        \"reviews\": \"\\n- \".join([\n",
    "                \"I bought a new phone, and now, after two months, the screen does not work, The only thing customer service can do for you is waiting music\",\n",
    "                \"I don't know what is going on with the battery of this device. I feel like I want to carry the charger wherever I go\"\n",
    "                ])\n",
    "    },\n",
    "    {\n",
    "        \"product\": \"vacuum cleaner\",\n",
    "        \"reviews\": \"\\n- \".join([\n",
    "                \"It has a voice beyond her capabilities\",\n",
    "                \"Very innovative. You will always need to use a broom after using it\",\n",
    "            ])\n",
    "    },\n",
    "]\n",
    "\n",
    "example_template = PromptTemplate(template= \"product : {product} \\n-----\\n reviews : {reviews}\"  , input_variables= ['product' , 'reviews'] )\n",
    "\n",
    "prompt = FewShotPromptTemplate(example_prompt = example_template  , examples = examples , suffix = \"question :{product} \\n----\\n\"  , input_variables = ['product'] ) \n",
    "message = prompt.format(product = \"computer\")\n",
    "print(model.invoke(message).content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hummus', 'shawarma', 'falafel']\n"
     ]
    }
   ],
   "source": [
    "### Output Parser : control on the output format \n",
    "## First Example : \n",
    "# message = \"give me the most three popular arabian plates , just plates without description\"\n",
    "# print(model.invoke(message).content) \n",
    "## how to handle this output ? \n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser \n",
    "from langchain.prompts.prompt import PromptTemplate \n",
    "### initialize output_parses \n",
    "output_parses = CommaSeparatedListOutputParser() \n",
    "format_instruction = output_parses.get_format_instructions() \n",
    "example_prompt = PromptTemplate(template= \"list threee popular {type} plates. \\n {format_instruction}\" , input_variables= ['type'] , partial_variables = {'format_instruction' : format_instruction} ) \n",
    "# give the template instruction format from format instruction from output parses , this make the ouput depend on the format instruction that given from commasperatedlsitoutputparser \n",
    "\n",
    "demo_message = example_prompt.format(type = 'arabian' )\n",
    "result = model.invoke(demo_message).content ## should use content \n",
    "output = output_parses.parse(result)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/06/1971 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## how to create manaually format inststruction without commaseperatedparser \n",
    "format_instruction = \"Replay with a dataTime format DAY/MONTH/YEAR like :'23/05/1999'. \".strip() \n",
    "prompt = PromptTemplate(template='when was {person} born? \\n{format_instruction}' , input_variables=['person' , 'format_instruction']) \n",
    "message  = prompt.format(person = 'Elon Musk'  , format_instruction = format_instruction ) \n",
    "print(model.invoke(message).content) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using pydantic to relate with ai respnose as a object oriented programming \n",
    "from pydantic import BaseModel , Field \n",
    "from langchain.output_parsers import PydanticOutputParser \n",
    "from typing import List ### to create list compatable with pydantic class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initial class to use it with pydantic \n",
    "class plate (BaseModel) : \n",
    "    plate_name : str  = Field(description='name of the plate') \n",
    "    ingrediants : List[str] = Field(description=\"list of names of the ingrediants \")\n",
    "parser  = PydanticOutputParser(pydantic_object= plate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['meat', 'pita bread', 'hummus', 'tahini', 'vegetables']\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "format_instruction = parser.get_format_instructions() \n",
    "template = PromptTemplate(template=\"Answer on the query. \\n{plate_name}\\n {format_instruction}\" , input_variables= ['plate_name'] , partial_variables = {'format_instruction' : format_instruction})  \n",
    "message = template.format(plate_name = 'shawarema') \n",
    "result = parser.parse(model.invoke(message).content) \n",
    "print(result.ingrediants) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the three places I listed, **Luxor Temple** has a view of the Nile River, which is essentially a large lake. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### give memory to model , in default model just response on the current  message  , don't have any memory about previouse question \n",
    "### how to give memroy to model using langchain \n",
    "### 1. using conversation Buffer memory \n",
    "from langchain.chains import ConversationChain \n",
    "from langchain.memory import ConversationBufferMemory \n",
    "from langchain_core.callbacks import StdOutCallbackHandler ### we use callback to get information about conversation to know the cost you must pay \n",
    "conversation = ConversationChain(llm = model , memory = ConversationBufferMemory() ) \n",
    "m1 = 'can you list 3 places in Egypt to visit ?\\n just places name '.strip() \n",
    "m2 = 'which place has a lake view ? '\n",
    "output = conversation.predict(input = m1 ) \n",
    "# print(output) \n",
    "print(conversation.predict(input = m2 )) \n",
    "### to print the conversation messages \n",
    "# for msg in conversation.memory.chat_memory.messages : \n",
    "#     print(msg) \n",
    "#     print('---------------------')\n",
    "    \n",
    "### to remove the memory \n",
    "# conversation.memroy.clear()\n",
    "\n",
    "### verbose : mean give tell me in detailed what happend behind , by default is False  \n",
    "# conversation.verbose = True   \n",
    "### there are a problem with this method (using conversationBufferMemory) expensive ,because  the prompt tokenize size large that make the cost high \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: can you list 3 places in Egypt to visit ?\n",
      " just places name\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: can you list 3 places in Egypt to visit ?\n",
      " just places name\n",
      "AI: Sure, here are 3 places in Egypt to visit:\n",
      "\n",
      "1.  **Giza Pyramid Complex**\n",
      "2.  **Luxor Temple**\n",
      "3.  **Abu Simbel** \n",
      "\n",
      "Human: which place has a lake view ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Of the three places I listed, **Luxor Temple** has a view of the Nile River.  It's located on the east bank of the Nile, and the river is a major feature of the temple complex.  \n",
      "\n",
      "While the Giza Pyramid Complex is near the Nile, it's not directly on the river and doesn't have a prominent view of it.  Abu Simbel is located far from the Nile, in the southern part of Egypt, and the view is dominated by the desert landscape. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### use alternative way (ConversationBufferWindowMemory)\n",
    "from langchain.memory import ConversationBufferWindowMemory \n",
    "## tell the model from the previous history take just this window \n",
    "chat = ConversationChain(llm = model , memory = ConversationBufferWindowMemory(k =1 ) , verbose = True   ) \n",
    "\n",
    "output1 = chat.predict(input = m1 )  \n",
    "# print(output1 ) \n",
    "output2 = chat.predict(input = m2 ) \n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Giza Necropolis\n",
      "2. Luxor Temple\n",
      "3. Abu Simbel \n",
      "\n",
      "Luxor Temple \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### summary conversation memory  ## the most cost memory  \n",
    "from langchain.memory import ConversationSummaryMemory \n",
    "chating = ConversationChain(llm = model , memory = ConversationSummaryMemory(llm = model)) \n",
    "out1 = chating.predict(input = m1) \n",
    "print(out1) \n",
    "\n",
    "out2 = chating.predict(input = f'{m2} , give me just name '  ) \n",
    "print(out2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### use Entity memory ŸÖŸÉŸÑŸÅ ÿ¨ÿØÿß ÿ®ÿ±ÿØŸá ÿπÿ¥ÿßŸÜ ÿ®Ÿäÿ≠ÿßŸàŸÑ Ÿäÿ¥ÿ±ÿ≠ ÿßÿ≤ÿßŸä ÿßŸÑŸÖŸàÿØŸäŸÑ Ÿäÿ≥ÿ™ÿÆÿ±ÿ¨ ÿßÿ≥ÿßŸÖŸä ÿßŸÑÿßÿ¥Ÿäÿßÿ° ŸÖŸÜ ÿßŸÑŸÜÿµ \n",
    "from langchain.memory import ConversationEntityMemory \n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE \n",
    "entity_chat = ConversationChain(llm = model , memory = ConversationEntityMemory(llm = model ) , prompt = ENTITY_MEMORY_CONVERSATION_TEMPLATE   ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save File(how to save conversation to use it in another position ) \n",
    "## we will use  conversationsummaryMemory\n",
    "from langchain.chains import ConversationChain \n",
    "from langchain.memory import ConversationSummaryMemory \n",
    "from langchain.schema import messages_from_dict  , messages_to_dict \n",
    "from typing import List \n",
    "import json \n",
    "summer_chat = ConversationChain(llm = model , memory = ConversationSummaryMemory(llm = model )  ) \n",
    "summer_output1 = summer_chat.predict(input = m1 ) \n",
    "# print(summer_output1) \n",
    "summer_output2 = summer_chat.predict(input = f'{m2} , give me just the name ') \n",
    "# print(summer_output2 ) \n",
    "dicts = messages_to_dict(summer_chat.memory.chat_memory.messages)\n",
    "\n",
    "with open('conversation_memory.json' , 'w') as dest : \n",
    "    dest.write(json.dumps( dicts , ensure_ascii= False )) ## to make the arabic letters appear in json file make ensure_ascii = false \n",
    "\n",
    "with open('conversation_memory.json' , 'r') as src : \n",
    "    saved_history = json.load(src) \n",
    "    \n",
    "## to use this history in next chat \n",
    "from langchain.memory import ChatMessageHistory \n",
    "history = ChatMessageHistory() \n",
    "history.messages = messages_from_dict(saved_history) ### must convert dictionary to message that can model understand that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From my suggestions, **Luxor Temple** is a good choice to visit in winter. \n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **Weather:** Winter in Egypt is generally pleasant with sunny days and cooler nights. Luxor experiences mild temperatures during this season, making it comfortable to explore the temple complex.\n",
      "* **Crowds:**  The winter months (November to February) are considered the \"off-season\" in Luxor, meaning there are fewer crowds compared to the summer months. This allows for a more peaceful and enjoyable experience.\n",
      "* **Festivals:** Luxor hosts several festivals and events during the winter, including the \"Luxor Winter Festival\" which features cultural performances, music, and traditional crafts.\n",
      "\n",
      "While the Giza Pyramids and Abu Simbel are also worth visiting, they can be quite hot during the summer months. Winter offers a more comfortable experience for exploring these ancient wonders. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### reuse this history in model \n",
    "\n",
    "memory = ConversationSummaryMemory(llm = model ) \n",
    "memory = memory.from_messages(chat_memory= history , llm  = model ) \n",
    "summer_chat = ConversationChain(llm = model , memory = memory ) \n",
    "summer_m3 = 'from your suggesion places ,  what is the place can visit it in winter ? ' \n",
    "print(summer_chat.predict(input = summer_m3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Hugging face (download model on your machine ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\root\\Programming\\Pratical_Ai\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer , LlamaForCausalLM , GenerationConfig , pipeline \n",
    "from langchain_huggingface import HuggingFacePipeline \n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "e:\\root\\Programming\\Pratical_Ai\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:579: UserWarning: `pad_token_id` should be positive but got -1. This will cause errors when batch generating, if there is padding. Please set `pad_token_id` explicitly as `model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.53it/s]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"chavinlo/alpaca-native\" \n",
    "## for loading the model with reduce size use two attributes load_in_8bit = true , device_map = 'auto' \n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id) \n",
    "base_model = LlamaForCausalLM.from_pretrained(model_id )\n",
    "\n",
    "### now , we create the base model : give hime the ability to predict the next word by causalLm  + give hime tokenizer table  (this two features founded in model id  , when install it ) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "# now , we want to create a pipline (the type of task , the model that you want to use it when create pipeline , specify the tokenizer ) . \n",
    "hug_pipeline = pipeline('text-generation' , model = base_model , tokenizer=  tokenizer  , max_length = 256 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create an instance from hugginface \n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hug_pipeline ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_instruction = input('what i can help you')\n",
    "prompt = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction: \n",
    "{user_instruction}\n",
    "Answer:\"\"\".strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction: \\nwhat is  your name ? \\nAnswer: My name is John.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I don't have a name. I am a large language model, and I am not a person. I am a computer program that can process and generate text. üòä \\n\" additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-cf3dedbf-9c02-4cab-815f-3b8cd3ec4500-0' usage_metadata={'input_tokens': 6, 'output_tokens': 35, 'total_tokens': 41}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
